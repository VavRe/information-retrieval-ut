{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zZsVoeyvTiyLVT2R1md4owi5-QKYuPfy",
      "authorship_tag": "ABX9TyO48xA7SqDe9II07TjWjZXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VavRe/information-retrieval-ut/blob/main/CA6/IR_CA6_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miZ8UsusoYWs",
        "outputId": "cfd5101d-1f20-4e8b-c26b-5e863b6c1416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 2.182 seconds.\n",
            "DEBUG:jieba:Loading model cost 2.182 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ],
      "source": [
        "from numpy import zeros, int8, log\n",
        "from pylab import random\n",
        "import sys\n",
        "import jieba\n",
        "import re\n",
        "import time\n",
        "import codecs\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "# segmentation, stopwords filtering and document-word matrix generating\n",
        "# [return]:\n",
        "# N : number of documents\n",
        "# M : length of dictionary\n",
        "# word2id : a map mapping terms to their corresponding ids\n",
        "# id2word : a map mapping ids to terms\n",
        "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
        "def preprocessing(datasetFilePath):\n",
        "\n",
        "    df=pd.read_csv('/content/drive/MyDrive/IR/BBC News.csv')\n",
        "    documents=df['News'].values.tolist()[0:50]\n",
        "\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    # number of documents\n",
        "    N = len(documents)\n",
        "\n",
        "    wordCounts = [];\n",
        "    word2id = {}\n",
        "    id2word = {}\n",
        "    currentId = 0;\n",
        "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
        "    for document in documents:\n",
        "        segList = jieba.cut(document)\n",
        "        wordCount = {}\n",
        "        for word in segList:\n",
        "            word = word.lower().strip()\n",
        "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stops:               \n",
        "                if word not in word2id.keys():\n",
        "                    word2id[word] = currentId;\n",
        "                    id2word[currentId] = word;\n",
        "                    currentId += 1;\n",
        "                if word in wordCount:\n",
        "                    wordCount[word] += 1\n",
        "                else:\n",
        "                    wordCount[word] = 1\n",
        "        wordCounts.append(wordCount);\n",
        "    \n",
        "    # length of dictionary\n",
        "    M = len(word2id)  \n",
        "\n",
        "    # generate the document-word matrix\n",
        "    X = zeros([N, M], int8)\n",
        "    for word in word2id.keys():\n",
        "        j = word2id[word]\n",
        "        for i in range(0, N):\n",
        "            if word in wordCounts[i]:\n",
        "                X[i, j] = wordCounts[i][word];   \n",
        "    #############\n",
        "    ## My Work ##\n",
        "    ############# \n",
        "    \n",
        "    bg_model = zeros([M])\n",
        "    corpus_words = sum(sum(X))\n",
        "    for i in range(0,M):\n",
        "        bg_model[i] = sum(X.T[i]) / corpus_words\n",
        "\n",
        "    return N, M, word2id, id2word, X, bg_model\n",
        "\n",
        "def initializeParameters():\n",
        "    for i in range(0, N):\n",
        "        normalization = sum(document_topic_dist[i, :])\n",
        "        for j in range(0, K):\n",
        "            document_topic_dist[i, j] /= normalization;\n",
        "\n",
        "    for i in range(0, K):\n",
        "        normalization = sum(topic_word_dist[i, :])\n",
        "        for j in range(0, M):\n",
        "            topic_word_dist[i, j] /= normalization;\n",
        "\n",
        "def EStep(bg_model,lamda_b):\n",
        "    for i in range(0, N):\n",
        "        # print(\"E\",i)\n",
        "        for j in range(0, M):\n",
        "            denominator = 0;\n",
        "            for k in range(0, K):\n",
        "                p[i, j, k] = topic_word_dist[k, j] * document_topic_dist[i, k];\n",
        "                denominator += p[i, j, k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] = 0;\n",
        "            else:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] /= denominator;\n",
        "            #############\n",
        "            ## My Work ##\n",
        "            ############# \n",
        "            denominator_bg = lamda_b * bg_model[j] + (1-lamda_b) * denominator\n",
        "            p_bg[i,j,0] = (lamda_b * bg_model[j] ) / denominator_bg\n",
        "\n",
        "def MStep():\n",
        "    # update topic_word_dist\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            topic_word_dist[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                #############\n",
        "                ## My Work ##\n",
        "                ############# \n",
        "                topic_word_dist[k, j] += X[i, j] * p[i, j, k] * (1 - p_bg[i,j,0] )\n",
        "            denominator += topic_word_dist[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                topic_word_dist[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                topic_word_dist[k, j] /= denominator\n",
        "        \n",
        "    # update document_topic_dist\n",
        "    for i in range(0, N):\n",
        "        for k in range(0, K):\n",
        "            document_topic_dist[i, k] = 0\n",
        "            denominator = 0\n",
        "            for j in range(0, M):\n",
        "                #############\n",
        "                ## My Work ##\n",
        "                #############\n",
        "                document_topic_dist[i, k] += X[i, j] * p[i, j, k] * (1 - p_bg[i,j,0] )\n",
        "                denominator += X[i, j];\n",
        "            if denominator == 0:\n",
        "                document_topic_dist[i, k] = 1.0 / K\n",
        "            else:\n",
        "                document_topic_dist[i, k] /= denominator\n",
        "\n",
        "# calculate the log likelihood\n",
        "def LogLikelihood(bg_model,lamda_b):\n",
        "    loglikelihood = 0\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            tmp = 0\n",
        "            for k in range(0, K):\n",
        "                tmp += topic_word_dist[k, j] * document_topic_dist[i, k]\n",
        "            #############\n",
        "            ## My Work ##\n",
        "            #############\n",
        "            tmp = lamda_b * bg_model[j]+(1-lamda_b)*tmp\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# output the params of model and top words of topics to files\n",
        "def output():\n",
        "    # document-topic distribution\n",
        "    file = codecs.open(docTopicDist,'w','utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(document_topic_dist[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "    \n",
        "    # topic-word distribution\n",
        "    file = codecs.open(topicWordDist,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(topic_word_dist[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "    \n",
        "    # dictionary\n",
        "    file = codecs.open(dictionary,'w','utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "    \n",
        "    # top words of each topic\n",
        "    file = codecs.open(topicWords,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = topic_word_dist[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "    \n",
        "# set the default params and read the params from cmd\n",
        "datasetFilePath = 'dataset.txt'\n",
        "# K = 20    # choose different number of topics\n",
        "maxIteration = 10\n",
        "threshold = 0.0001\n",
        "topicWordsNum = 10\n",
        "dictionary = 'dictionary.dic'\n",
        "docTopicDist = f'docTopicDistribution-k={K}-lamda-{lamda_b}.txt'\n",
        "topicWordDist = f'topicWordDistribution-k={K}-lamda={lamda_b}.txt'\n",
        "topicWords = f'topics-k={K}-lamda={lamda_b}.txt'\n",
        "lamda_b = 0.3\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "N, M, word2id, id2word, X , bg_model = preprocessing(datasetFilePath)\n",
        "\n",
        "# # document_topic_dist[i, j] : p(zj|di)\n",
        "# document_topic_dist = random([N, K])\n",
        "\n",
        "# # topic_word_dist[i, j] : p(wj|zi)\n",
        "# topic_word_dist = random([K, M])\n",
        "\n",
        "# # p[i, j, k] : p(zk|di,wj)\n",
        "# p = zeros([N, M, K])\n",
        "# p_bg = zeros([N,M,1])\n",
        "\n",
        "# initializeParameters()\n",
        "\n",
        "# # EM algorithm\n",
        "# oldLoglikelihood = 1\n",
        "# newLoglikelihood = 1\n",
        "# for i in range(0, maxIteration):\n",
        "#     EStep(bg_model,lamda_b)\n",
        "#     MStep()\n",
        "#     newLoglikelihood = LogLikelihood(bg_model,lamda_b)\n",
        "#     print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "#     if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "#         break\n",
        "#     oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "# output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_counts = [5,10,20,30,50]\n",
        "lamdas = [0,0.4,0.8,1]\n",
        "for K in topic_counts:\n",
        "  for lamda_b in lamdas:\n",
        "    docTopicDist = f'/content/drive/MyDrive/IR/results/docTopicDistribution-k={K}-lamda-{lamda_b}.txt'\n",
        "    topicWordDist = f'/content/drive/MyDrive/IR/results/topicWordDistribution-k={K}-lamda={lamda_b}.txt'\n",
        "    topicWords = f'/content/drive/MyDrive/IR/results/topics-k={K}-lamda={lamda_b}.txt'\n",
        "    print(f\"k is {K} and lamda is {lamda_b}\")\n",
        "    # document_topic_dist[i, j] : p(zj|di)\n",
        "    document_topic_dist = random([N, K])\n",
        "\n",
        "    # topic_word_dist[i, j] : p(wj|zi)\n",
        "    topic_word_dist = random([K, M])\n",
        "\n",
        "    # p[i, j, k] : p(zk|di,wj)\n",
        "    p = zeros([N, M, K])\n",
        "    p_bg = zeros([N,M,1])\n",
        "\n",
        "    initializeParameters()\n",
        "\n",
        "    # EM algorithm\n",
        "    oldLoglikelihood = 1\n",
        "    newLoglikelihood = 1\n",
        "    for i in range(0, maxIteration):\n",
        "        EStep(bg_model,lamda_b)\n",
        "        MStep()\n",
        "        newLoglikelihood = LogLikelihood(bg_model,lamda_b)\n",
        "        print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "        if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "            break\n",
        "        oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "    output()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2IsTPhgEQc-",
        "outputId": "a1bb785c-540e-4e18-b444-251f327bea54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k is 5 and lamda is 0\n",
            "[ 2023-01-20 17:05:19 ]  1  iteration   -77620.66565187421\n",
            "[ 2023-01-20 17:05:29 ]  2  iteration   -76649.19354944075\n",
            "[ 2023-01-20 17:05:39 ]  3  iteration   -75539.78388459676\n",
            "[ 2023-01-20 17:05:49 ]  4  iteration   -74382.67862632895\n",
            "[ 2023-01-20 17:06:00 ]  5  iteration   -73288.62267050912\n",
            "[ 2023-01-20 17:06:10 ]  6  iteration   -72321.62350657741\n",
            "[ 2023-01-20 17:06:20 ]  7  iteration   -71546.27531316585\n",
            "[ 2023-01-20 17:06:30 ]  8  iteration   -70988.74966550535\n",
            "[ 2023-01-20 17:06:40 ]  9  iteration   -70610.68348530716\n",
            "[ 2023-01-20 17:06:50 ]  10  iteration   -70353.41016259546\n",
            "k is 5 and lamda is 0.4\n",
            "[ 2023-01-20 17:07:00 ]  1  iteration   -82335.68408022719\n",
            "[ 2023-01-20 17:07:12 ]  2  iteration   -82173.06229217949\n",
            "[ 2023-01-20 17:07:22 ]  3  iteration   -81218.25538631281\n",
            "[ 2023-01-20 17:07:32 ]  4  iteration   -79866.67212241674\n",
            "[ 2023-01-20 17:07:42 ]  5  iteration   -78544.37968169138\n",
            "[ 2023-01-20 17:07:52 ]  6  iteration   -77538.89349976394\n",
            "[ 2023-01-20 17:08:02 ]  7  iteration   -76841.52044954893\n",
            "[ 2023-01-20 17:08:12 ]  8  iteration   -76338.7520257413\n",
            "[ 2023-01-20 17:08:22 ]  9  iteration   -75956.3038514164\n",
            "[ 2023-01-20 17:08:32 ]  10  iteration   -75662.10640344596\n",
            "k is 5 and lamda is 0.8\n",
            "[ 2023-01-20 17:08:42 ]  1  iteration   -80459.35627262884\n",
            "[ 2023-01-20 17:08:52 ]  2  iteration   -80747.17068361444\n",
            "k is 5 and lamda is 1\n",
            "[ 2023-01-20 17:09:02 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 17:09:12 ]  2  iteration   -78678.92619935468\n",
            "k is 10 and lamda is 0\n",
            "[ 2023-01-20 17:09:31 ]  1  iteration   -77560.25653486546\n",
            "[ 2023-01-20 17:09:49 ]  2  iteration   -76234.75809502717\n",
            "[ 2023-01-20 17:10:08 ]  3  iteration   -74421.56320105228\n",
            "[ 2023-01-20 17:10:29 ]  4  iteration   -72310.08160106331\n",
            "[ 2023-01-20 17:10:48 ]  5  iteration   -70257.59444309722\n",
            "[ 2023-01-20 17:11:07 ]  6  iteration   -68541.7161011681\n",
            "[ 2023-01-20 17:11:25 ]  7  iteration   -67285.39634022953\n",
            "[ 2023-01-20 17:11:44 ]  8  iteration   -66434.33372683954\n",
            "[ 2023-01-20 17:12:03 ]  9  iteration   -65854.26753122697\n",
            "[ 2023-01-20 17:12:22 ]  10  iteration   -65441.77830569122\n",
            "k is 10 and lamda is 0.4\n",
            "[ 2023-01-20 17:12:41 ]  1  iteration   -82257.43984478632\n",
            "[ 2023-01-20 17:13:00 ]  2  iteration   -81994.79038720598\n",
            "[ 2023-01-20 17:13:18 ]  3  iteration   -80662.69274487197\n",
            "[ 2023-01-20 17:13:39 ]  4  iteration   -78471.43034700016\n",
            "[ 2023-01-20 17:13:58 ]  5  iteration   -75988.99009024122\n",
            "[ 2023-01-20 17:14:17 ]  6  iteration   -73822.36214132594\n",
            "[ 2023-01-20 17:14:36 ]  7  iteration   -72208.77258476555\n",
            "[ 2023-01-20 17:14:55 ]  8  iteration   -71139.91843838198\n",
            "[ 2023-01-20 17:15:14 ]  9  iteration   -70481.4229037741\n",
            "[ 2023-01-20 17:15:32 ]  10  iteration   -70081.87183236777\n",
            "k is 10 and lamda is 0.8\n",
            "[ 2023-01-20 17:15:51 ]  1  iteration   -80449.32540867833\n",
            "[ 2023-01-20 17:16:10 ]  2  iteration   -80757.20295849224\n",
            "k is 10 and lamda is 1\n",
            "[ 2023-01-20 17:16:28 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 17:16:48 ]  2  iteration   -78678.92619935468\n",
            "k is 20 and lamda is 0\n",
            "[ 2023-01-20 17:17:25 ]  1  iteration   -77499.50777183993\n",
            "[ 2023-01-20 17:18:02 ]  2  iteration   -76122.71424486862\n",
            "[ 2023-01-20 17:18:38 ]  3  iteration   -74103.49106151209\n",
            "[ 2023-01-20 17:19:14 ]  4  iteration   -71557.62098020659\n",
            "[ 2023-01-20 17:19:50 ]  5  iteration   -68831.33288505544\n",
            "[ 2023-01-20 17:20:28 ]  6  iteration   -66249.95828489414\n",
            "[ 2023-01-20 17:21:05 ]  7  iteration   -64106.58408195549\n",
            "[ 2023-01-20 17:21:41 ]  8  iteration   -62538.27153987976\n",
            "[ 2023-01-20 17:22:17 ]  9  iteration   -61467.27672577803\n",
            "[ 2023-01-20 17:22:53 ]  10  iteration   -60747.28973818844\n",
            "k is 20 and lamda is 0.4\n",
            "[ 2023-01-20 17:23:31 ]  1  iteration   -82200.57160027062\n",
            "[ 2023-01-20 17:24:07 ]  2  iteration   -81824.75170903343\n",
            "[ 2023-01-20 17:24:44 ]  3  iteration   -80163.38360919755\n",
            "[ 2023-01-20 17:25:20 ]  4  iteration   -77376.0008153837\n",
            "[ 2023-01-20 17:25:56 ]  5  iteration   -74120.80349487983\n",
            "[ 2023-01-20 17:26:34 ]  6  iteration   -71148.24813328762\n",
            "[ 2023-01-20 17:27:10 ]  7  iteration   -68814.0334715664\n",
            "[ 2023-01-20 17:27:46 ]  8  iteration   -67157.35553582001\n",
            "[ 2023-01-20 17:28:22 ]  9  iteration   -66100.35368417916\n",
            "[ 2023-01-20 17:28:58 ]  10  iteration   -65453.608854471364\n",
            "k is 20 and lamda is 0.8\n",
            "[ 2023-01-20 17:29:34 ]  1  iteration   -80450.5594075342\n",
            "[ 2023-01-20 17:30:13 ]  2  iteration   -80764.27563372665\n",
            "k is 20 and lamda is 1\n",
            "[ 2023-01-20 17:30:49 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 17:31:25 ]  2  iteration   -78678.92619935468\n",
            "k is 30 and lamda is 0\n",
            "[ 2023-01-20 17:32:18 ]  1  iteration   -77484.0675277257\n",
            "[ 2023-01-20 17:33:14 ]  2  iteration   -76058.96450562158\n",
            "[ 2023-01-20 17:34:08 ]  3  iteration   -73865.17480300915\n",
            "[ 2023-01-20 17:35:02 ]  4  iteration   -70913.49512532179\n",
            "[ 2023-01-20 17:35:55 ]  5  iteration   -67657.56665820115\n",
            "[ 2023-01-20 17:36:51 ]  6  iteration   -64644.23409596843\n",
            "[ 2023-01-20 17:37:44 ]  7  iteration   -62138.18186947879\n",
            "[ 2023-01-20 17:38:40 ]  8  iteration   -60237.966641307044\n",
            "[ 2023-01-20 17:39:39 ]  9  iteration   -58922.95295276942\n",
            "[ 2023-01-20 17:40:33 ]  10  iteration   -58038.03307636343\n",
            "k is 30 and lamda is 0.4\n",
            "[ 2023-01-20 17:41:32 ]  1  iteration   -82238.34919513037\n",
            "[ 2023-01-20 17:42:27 ]  2  iteration   -81940.59588716063\n",
            "[ 2023-01-20 17:43:21 ]  3  iteration   -80385.02118455041\n",
            "[ 2023-01-20 17:44:14 ]  4  iteration   -77444.29173043984\n",
            "[ 2023-01-20 17:45:13 ]  5  iteration   -73441.44698695616\n",
            "[ 2023-01-20 17:46:10 ]  6  iteration   -69464.92822458767\n",
            "[ 2023-01-20 17:47:08 ]  7  iteration   -66421.79582104547\n",
            "[ 2023-01-20 17:48:01 ]  8  iteration   -64400.0770599983\n",
            "[ 2023-01-20 17:48:58 ]  9  iteration   -63047.5097173084\n",
            "[ 2023-01-20 17:49:53 ]  10  iteration   -62129.58286630148\n",
            "k is 30 and lamda is 0.8\n",
            "[ 2023-01-20 17:50:48 ]  1  iteration   -80452.58593313234\n",
            "[ 2023-01-20 17:51:41 ]  2  iteration   -80771.9932328339\n",
            "k is 30 and lamda is 1\n",
            "[ 2023-01-20 17:52:37 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 17:53:30 ]  2  iteration   -78678.92619935468\n",
            "k is 50 and lamda is 0\n",
            "[ 2023-01-20 17:54:58 ]  1  iteration   -77472.5650501318\n",
            "[ 2023-01-20 17:56:29 ]  2  iteration   -76084.45556253662\n",
            "[ 2023-01-20 17:57:57 ]  3  iteration   -73949.54283936479\n",
            "[ 2023-01-20 17:59:26 ]  4  iteration   -70876.08040270642\n",
            "[ 2023-01-20 18:00:55 ]  5  iteration   -67145.38284124924\n",
            "[ 2023-01-20 18:02:25 ]  6  iteration   -63404.98543057189\n",
            "[ 2023-01-20 18:03:53 ]  7  iteration   -60191.551627211644\n",
            "[ 2023-01-20 18:05:22 ]  8  iteration   -57783.11373648875\n",
            "[ 2023-01-20 18:06:51 ]  9  iteration   -56109.41757142161\n",
            "[ 2023-01-20 18:08:22 ]  10  iteration   -54986.25650019797\n",
            "k is 50 and lamda is 0.4\n",
            "[ 2023-01-20 18:09:50 ]  1  iteration   -82206.1251458424\n",
            "[ 2023-01-20 18:11:20 ]  2  iteration   -81919.80098202673\n",
            "[ 2023-01-20 18:12:47 ]  3  iteration   -80401.8048868598\n",
            "[ 2023-01-20 18:14:16 ]  4  iteration   -77387.19294728225\n",
            "[ 2023-01-20 18:15:44 ]  5  iteration   -72985.17216174684\n",
            "[ 2023-01-20 18:17:12 ]  6  iteration   -68438.40512591379\n",
            "[ 2023-01-20 18:18:39 ]  7  iteration   -64937.47712449386\n",
            "[ 2023-01-20 18:20:09 ]  8  iteration   -62549.21827503677\n",
            "[ 2023-01-20 18:21:36 ]  9  iteration   -60957.5859657484\n",
            "[ 2023-01-20 18:23:05 ]  10  iteration   -59880.07357114083\n",
            "k is 50 and lamda is 0.8\n",
            "[ 2023-01-20 18:24:34 ]  1  iteration   -80457.49127196615\n",
            "[ 2023-01-20 18:26:02 ]  2  iteration   -80777.79884588251\n",
            "k is 50 and lamda is 1\n",
            "[ 2023-01-20 18:27:32 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 18:28:58 ]  2  iteration   -78678.92619935468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Calculate lamda effect on LogLikelihood"
      ],
      "metadata": {
        "id": "KcTKO5Cfx5wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_counts = [5]\n",
        "lamdas = [0,0.3,0.4,0.5,0.7,0.8,1]\n",
        "for K in topic_counts:\n",
        "  for lamda_b in lamdas:\n",
        "    docTopicDist = f'/content/drive/MyDrive/IR/results/docTopicDistribution-k={K}-lamda-{lamda_b}.txt'\n",
        "    topicWordDist = f'/content/drive/MyDrive/IR/results/topicWordDistribution-k={K}-lamda={lamda_b}.txt'\n",
        "    topicWords = f'/content/drive/MyDrive/IR/results/topics-k={K}-lamda={lamda_b}.txt'\n",
        "    print(f\"k is {K} and lamda is {lamda_b}\")\n",
        "    # document_topic_dist[i, j] : p(zj|di)\n",
        "    document_topic_dist = random([N, K])\n",
        "\n",
        "    # topic_word_dist[i, j] : p(wj|zi)\n",
        "    topic_word_dist = random([K, M])\n",
        "\n",
        "    # p[i, j, k] : p(zk|di,wj)\n",
        "    p = zeros([N, M, K])\n",
        "    p_bg = zeros([N,M,1])\n",
        "\n",
        "    initializeParameters()\n",
        "\n",
        "    # EM algorithm\n",
        "    oldLoglikelihood = 1\n",
        "    newLoglikelihood = 1\n",
        "    for i in range(0, maxIteration):\n",
        "        EStep(bg_model,lamda_b)\n",
        "        MStep()\n",
        "        newLoglikelihood = LogLikelihood(bg_model,lamda_b)\n",
        "        print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "        if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "            break\n",
        "        oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "    output()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ngfMa1gh8QU",
        "outputId": "e8d176c1-4931-4283-9889-73f8094f4aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k is 5 and lamda is 0\n",
            "[ 2023-01-20 19:41:14 ]  1  iteration   -77775.05699560874\n",
            "[ 2023-01-20 19:41:24 ]  2  iteration   -76801.29270465448\n",
            "[ 2023-01-20 19:41:35 ]  3  iteration   -75591.45614928237\n",
            "[ 2023-01-20 19:41:45 ]  4  iteration   -74298.23885814568\n",
            "[ 2023-01-20 19:41:55 ]  5  iteration   -73131.81939371089\n",
            "[ 2023-01-20 19:42:06 ]  6  iteration   -72178.1196733101\n",
            "[ 2023-01-20 19:42:18 ]  7  iteration   -71420.58818584416\n",
            "[ 2023-01-20 19:42:28 ]  8  iteration   -70848.59718684759\n",
            "[ 2023-01-20 19:42:39 ]  9  iteration   -70464.13215534034\n",
            "[ 2023-01-20 19:42:49 ]  10  iteration   -70219.93080239034\n",
            "k is 5 and lamda is 0.3\n",
            "[ 2023-01-20 19:42:59 ]  1  iteration   -82349.959986092\n",
            "[ 2023-01-20 19:43:10 ]  2  iteration   -81441.92216079264\n",
            "[ 2023-01-20 19:43:20 ]  3  iteration   -79958.56023322252\n",
            "[ 2023-01-20 19:43:30 ]  4  iteration   -78301.97740443311\n",
            "[ 2023-01-20 19:43:40 ]  5  iteration   -76804.63201446735\n",
            "[ 2023-01-20 19:43:51 ]  6  iteration   -75612.69950110362\n",
            "[ 2023-01-20 19:44:01 ]  7  iteration   -74727.43081882292\n",
            "[ 2023-01-20 19:44:11 ]  8  iteration   -74106.97726298127\n",
            "[ 2023-01-20 19:44:21 ]  9  iteration   -73675.51298852013\n",
            "[ 2023-01-20 19:44:31 ]  10  iteration   -73378.8178997773\n",
            "k is 5 and lamda is 0.4\n",
            "[ 2023-01-20 19:44:41 ]  1  iteration   -82410.02876622693\n",
            "[ 2023-01-20 19:44:52 ]  2  iteration   -82338.86338481837\n",
            "[ 2023-01-20 19:45:04 ]  3  iteration   -81433.95552308358\n",
            "[ 2023-01-20 19:45:14 ]  4  iteration   -80011.7468959761\n",
            "[ 2023-01-20 19:45:24 ]  5  iteration   -78549.91726590562\n",
            "[ 2023-01-20 19:45:35 ]  6  iteration   -77344.5326500539\n",
            "[ 2023-01-20 19:45:45 ]  7  iteration   -76417.49788957962\n",
            "[ 2023-01-20 19:45:55 ]  8  iteration   -75749.11273568352\n",
            "[ 2023-01-20 19:46:05 ]  9  iteration   -75316.5453901949\n",
            "[ 2023-01-20 19:46:15 ]  10  iteration   -75060.39199045015\n",
            "k is 5 and lamda is 0.5\n",
            "[ 2023-01-20 19:46:26 ]  1  iteration   -82234.79092528188\n",
            "[ 2023-01-20 19:46:36 ]  2  iteration   -82702.04067432437\n",
            "k is 5 and lamda is 0.7\n",
            "[ 2023-01-20 19:46:47 ]  1  iteration   -81169.06109824168\n",
            "[ 2023-01-20 19:46:57 ]  2  iteration   -81645.42789628041\n",
            "k is 5 and lamda is 0.8\n",
            "[ 2023-01-20 19:47:07 ]  1  iteration   -80478.60288253603\n",
            "[ 2023-01-20 19:47:17 ]  2  iteration   -80789.10313887906\n",
            "k is 5 and lamda is 1\n",
            "[ 2023-01-20 19:47:28 ]  1  iteration   -78678.92619935468\n",
            "[ 2023-01-20 19:47:38 ]  2  iteration   -78678.92619935468\n"
          ]
        }
      ]
    }
  ]
}